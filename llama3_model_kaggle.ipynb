{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T10:32:00.450646Z",
     "iopub.status.busy": "2025-08-19T10:32:00.450327Z",
     "iopub.status.idle": "2025-08-19T10:32:04.565562Z",
     "shell.execute_reply": "2025-08-19T10:32:04.564726Z",
     "shell.execute_reply.started": "2025-08-19T10:32:00.450625Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-bigquery>=3.31.0 in /usr/local/lib/python3.11/dist-packages (3.35.1)\n",
      "Requirement already satisfied: google-cloud-bigquery-storage<3.0.0,>=2.30.0 in /usr/local/lib/python3.11/dist-packages (2.32.0)\n",
      "Requirement already satisfied: google-api-core<3.0.0,>=2.11.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery>=3.31.0) (2.25.1)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery>=3.31.0) (2.40.3)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery>=3.31.0) (2.4.3)\n",
      "Requirement already satisfied: google-resumable-media<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery>=3.31.0) (2.7.2)\n",
      "Requirement already satisfied: packaging>=24.2.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery>=3.31.0) (25.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery>=3.31.0) (2.9.0.post0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery>=3.31.0) (2.32.4)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery-storage<3.0.0,>=2.30.0) (1.26.1)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery-storage<3.0.0,>=2.30.0) (4.25.3)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=2.11.1->google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery>=3.31.0) (1.70.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery>=3.31.0) (1.74.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery>=3.31.0) (1.74.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-bigquery>=3.31.0) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-bigquery>=3.31.0) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-bigquery>=3.31.0) (4.9.1)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.11/dist-packages (from google-resumable-media<3.0.0,>=2.0.0->google-cloud-bigquery>=3.31.0) (1.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.8.2->google-cloud-bigquery>=3.31.0) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.21.0->google-cloud-bigquery>=3.31.0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.21.0->google-cloud-bigquery>=3.31.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.21.0->google-cloud-bigquery>=3.31.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.21.0->google-cloud-bigquery>=3.31.0) (2025.6.15)\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 (from google-cloud-bigquery-storage<3.0.0,>=2.30.0)\n",
      "  Using cached protobuf-6.32.0-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-cloud-bigquery>=3.31.0) (0.6.1)\n",
      "Using cached protobuf-6.32.0-cp39-abi3-manylinux2014_x86_64.whl (322 kB)\n",
      "Installing collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.3\n",
      "    Uninstalling protobuf-4.25.3:\n",
      "      Successfully uninstalled protobuf-4.25.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.25.1 which is incompatible.\n",
      "google-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 6.32.0 which is incompatible.\n",
      "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.32.0 which is incompatible.\n",
      "tensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.32.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed protobuf-6.32.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade \"google-cloud-bigquery>=3.31.0\" \"google-cloud-bigquery-storage>=2.30.0,<3.0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T10:32:04.567274Z",
     "iopub.status.busy": "2025-08-19T10:32:04.567027Z",
     "iopub.status.idle": "2025-08-19T10:32:08.473495Z",
     "shell.execute_reply": "2025-08-19T10:32:08.472673Z",
     "shell.execute_reply.started": "2025-08-19T10:32:04.567250Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "grpcio-status 1.74.0 requires protobuf<7.0.0,>=6.31.1, but you have protobuf 4.25.3 which is incompatible.\n",
      "google-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.25.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install protobuf==4.25.3 -q --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T10:32:08.474702Z",
     "iopub.status.busy": "2025-08-19T10:32:08.474486Z",
     "iopub.status.idle": "2025-08-19T10:32:11.689440Z",
     "shell.execute_reply": "2025-08-19T10:32:11.688466Z",
     "shell.execute_reply.started": "2025-08-19T10:32:08.474678Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rich<14,>=12.4.4 in /usr/local/lib/python3.11/dist-packages (13.9.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=12.4.4) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=12.4.4) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=12.4.4) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"rich<14,>=12.4.4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T10:32:11.692210Z",
     "iopub.status.busy": "2025-08-19T10:32:11.691974Z",
     "iopub.status.idle": "2025-08-19T10:33:32.604526Z",
     "shell.execute_reply": "2025-08-19T10:33:32.603507Z",
     "shell.execute_reply.started": "2025-08-19T10:32:11.692186Z"
    },
    "executionInfo": {
     "elapsed": 5241,
     "status": "ok",
     "timestamp": 1755503759722,
     "user": {
      "displayName": "권순민",
      "userId": "01464331670350596051"
     },
     "user_tz": -540
    },
    "id": "V1BR7JhKrg5x",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -qU transformers datasets accelerate peft bitsandbytes trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T10:33:32.605929Z",
     "iopub.status.busy": "2025-08-19T10:33:32.605658Z",
     "iopub.status.idle": "2025-08-19T10:33:58.839781Z",
     "shell.execute_reply": "2025-08-19T10:33:58.838888Z",
     "shell.execute_reply.started": "2025-08-19T10:33:32.605893Z"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1755503759741,
     "user": {
      "displayName": "권순민",
      "userId": "01464331670350596051"
     },
     "user_tz": -540
    },
    "id": "Rmdh5Ctw4XyP",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-19 10:33:41.560441: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1755599621.825867      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1755599621.903298      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T10:33:58.840839Z",
     "iopub.status.busy": "2025-08-19T10:33:58.840615Z",
     "iopub.status.idle": "2025-08-19T10:33:58.844449Z",
     "shell.execute_reply": "2025-08-19T10:33:58.843776Z",
     "shell.execute_reply.started": "2025-08-19T10:33:58.840822Z"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1755503759749,
     "user": {
      "displayName": "권순민",
      "userId": "01464331670350596051"
     },
     "user_tz": -540
    },
    "id": "ZXG1q7hyy6w3",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-08-19T10:33:58.845547Z",
     "iopub.status.busy": "2025-08-19T10:33:58.845270Z",
     "iopub.status.idle": "2025-08-19T10:33:58.953311Z",
     "shell.execute_reply": "2025-08-19T10:33:58.952628Z",
     "shell.execute_reply.started": "2025-08-19T10:33:58.845530Z"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1755503759760,
     "user": {
      "displayName": "권순민",
      "userId": "01464331670350596051"
     },
     "user_tz": -540
    },
    "id": "yWbof1dAsIFk",
    "outputId": "c9c2ccaf-6bbc-4b6f-8577-12afacb81707",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': '네, 명확한 3년 성장 계획을 가지고 있습니다. 크게 세 가지 목표입니다. 첫째, 현재 제가 주력으로 다루는 백엔드 '\n",
      "           '시스템 아키텍처 역량을 고도화하여, 대규모 트래픽 분산 처리 및 고가용성 설계 전문가가 되는 것입니다. 특히, '\n",
      "           'MSA(Microservices Architecture) 환경에서 서비스 간 트랜잭션 일관성 유지 및 분산 추적 '\n",
      "           '시스템(Distributed Tracing) 구축 경험을 심화하고 싶습니다. 이를 위해 특정 클라우드 플랫폼(예: '\n",
      "           'AWS Solution Architect Professional) 인증 취득 및 관련 오픈소스 프로젝트에 기여하는 것을 '\n",
      "           '목표로 하고 있습니다. 둘째, 데이터 기반 의사결정 역량을 강화하여, 개발하는 서비스의 사용자 행동 패턴 분석 및 '\n",
      "           '비즈니스 성과 지표 도출에 직접적으로 기여하고자 합니다. 이를 위해 데이터 파이프라인 구축 및 시각화 도구 활용 능력을 '\n",
      "           '향상시키고, AB 테스트 설계 및 분석 리더 역할을 수행하고 싶습니다. 셋째, 기술 리더십 역량을 키워 주니어 개발자 '\n",
      "           '멘토링 및 팀 내 기술 스택 도입을 주도하는 역할을 하고 싶습니다. 단순히 코드를 잘 짜는 것을 넘어, 팀원들의 성장을 '\n",
      "           '돕고 기술 부채를 줄이며, 장기적인 아키텍처 방향성을 제시하는 시니어 엔지니어로 자리매김하고 싶습니다. 이 목표들을 '\n",
      "           '달성하기 위해 지속적으로 학습하고, 회사에 실질적인 가치를 제공할 수 있도록 노력할 것입니다.',\n",
      " 'feedback': \"3년 성장 계획을 '세 가지 목표'로 명확히 구조화하여 제시한 점이 매우 인상 깊습니다. 각 목표가 백엔드 아키텍처 \"\n",
      "             '고도화, 데이터 기반 의사결정 역량 강화, 기술 리더십 역량 강화 등 시니어 엔지니어로서 필요한 핵심 역량을 고르게 '\n",
      "             '아우르고 있습니다. 특히, MSA 환경에서의 트랜잭션 일관성, 분산 추적 시스템 구축, AWS Solution '\n",
      "             'Architect Professional 인증 취득, AB 테스트 설계 및 분석 리더 역할 수행 등 구체적인 기술 '\n",
      "             '스택과 달성 방법론까지 제시하여 논리성과 구체성이 뛰어납니다. 단순히 기술 역량 향상에 그치지 않고, 비즈니스 성과 '\n",
      "             \"기여 및 팀원 성장 멘토링, 기술 부채 관리 등 '시니어'의 역할을 정확히 이해하고 있다는 점이 직무 적합성에 매우 \"\n",
      "             '긍정적인 평가를 주었습니다. 지원자의 성장 의지와 계획의 깊이가 돋보이는 답변입니다.',\n",
      " 'improve': '제시된 성장 계획의 내용과 깊이는 매우 훌륭합니다. 한 가지 더 보완할 점이 있다면, 이러한 성장 목표들이 지원한 '\n",
      "            '회사의 특정 비전이나 현재 당면한 기술적, 비즈니스적 도전과제와 어떻게 시너지를 낼 수 있을지에 대한 연결고리를 더욱 '\n",
      "            \"명확히 제시한다면, 단순히 개인의 성장을 넘어 '이 회사에서' 어떤 가치를 창출할지에 대한 의지를 더욱 부각시킬 수 \"\n",
      "            \"있을 것입니다. (예: '저의 고가용성 아키텍처 전문성은 회사의 특정 서비스 확장 계획에 기여할 수 있습니다.')\",\n",
      " 'question': '지원자님께서 지금부터 삼 년 뒤까지 즉 삼 년 이내로 주요 성장 계획이나 목표가 있으신가요 이러한 목표가 있다면 '\n",
      "             '무엇인지 말씀해 주시겠습니까',\n",
      " 'score': 4.8}\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 설정\n",
    "with open(\"/kaggle/input/json-data/interview_json_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    interview_json_data = json.load(f)\n",
    "\n",
    "pprint(interview_json_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T10:33:58.954396Z",
     "iopub.status.busy": "2025-08-19T10:33:58.954079Z",
     "iopub.status.idle": "2025-08-19T10:33:58.959722Z",
     "shell.execute_reply": "2025-08-19T10:33:58.958972Z",
     "shell.execute_reply.started": "2025-08-19T10:33:58.954377Z"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1755503759795,
     "user": {
      "displayName": "권순민",
      "userId": "01464331670350596051"
     },
     "user_tz": -540
    },
    "id": "_bLx73iQ05a7",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 데이터셋 llama3 에 맞게 설정\n",
    "def create_format_dataset(json_data_list):\n",
    "    formatted_data = []\n",
    "\n",
    "    system_prompt = \"\"\"당신은 전문 면접 평가자입니다. 당신의 임무는 제공된 면접 질문과 지원자의 답변을 분석하는 것입니다.\n",
    "이 분석을 바탕으로 'score', 'feedback', 'improve'이라는 세 가지 키를 포함하는 JSON 객체를 생성해야 합니다.\n",
    "- 'score': 답을 평가하는 숫자 점수 (1.0 ~ 5.0).\n",
    "- 'feedback': 답변의 강점에 대한 상세하고 건설적인 피드백.\n",
    "- 'improve': 답변에서 개선할 수 있는 사항에 대한 제안.\"\"\"\n",
    "    \n",
    "    for item in json_data_list:\n",
    "        # 'user' 부분: 질문과 답변\n",
    "        user_content = f\"\"\"\n",
    "        ### 질문:\n",
    "        {item['question']}\n",
    "        ### 답변:\n",
    "        {item['answer']}\"\"\"\n",
    "\n",
    "        # 'assistant' 부분: 평가 JSON\n",
    "        assistant_content_dict = {\n",
    "            \"score\": item['score'],\n",
    "            \"feedback\": item['feedback'],\n",
    "            \"improve\": item['improve']\n",
    "        }\n",
    "        assistant_content = json.dumps(assistant_content_dict, ensure_ascii=False, indent=2)\n",
    "\n",
    "        # Llama 3 템플릿에 맞게 messages 리스트 구성\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_content},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "        ]\n",
    "\n",
    "        formatted_data.append({\"messages\": messages})\n",
    "\n",
    "    return formatted_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-08-19T10:33:58.960960Z",
     "iopub.status.busy": "2025-08-19T10:33:58.960716Z",
     "iopub.status.idle": "2025-08-19T10:33:59.093171Z",
     "shell.execute_reply": "2025-08-19T10:33:59.092416Z",
     "shell.execute_reply.started": "2025-08-19T10:33:58.960942Z"
    },
    "executionInfo": {
     "elapsed": 99,
     "status": "ok",
     "timestamp": 1755503759908,
     "user": {
      "displayName": "권순민",
      "userId": "01464331670350596051"
     },
     "user_tz": -540
    },
    "id": "9mJIBVTf22_M",
    "outputId": "9668ee2e-4cb2-4098-bdaf-0edafba49d86",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': \"당신은 전문 면접 평가자입니다. 당신의 임무는 제공된 면접 질문과 지원자의 답변을 분석하는 것입니다.\\n이 분석을 바탕으로 'score', 'feedback', 'improve'이라는 세 가지 키를 포함하는 JSON 객체를 생성해야 합니다.\\n- 'score': 답을 평가하는 숫자 점수.\\n- 'feedback': 답변의 강점에 대한 상세하고 건설적인 피드백.\\n- 'improve': 답변에서 개선할 수 있는 사항에 대한 제안.\", 'role': 'system'}, {'content': '\\n        ### 질문:\\n        지원자님께서 지금부터 삼 년 뒤까지 즉 삼 년 이내로 주요 성장 계획이나 목표가 있으신가요 이러한 목표가 있다면 무엇인지 말씀해 주시겠습니까\\n        ### 답변:\\n        네, 명확한 3년 성장 계획을 가지고 있습니다. 크게 세 가지 목표입니다. 첫째, 현재 제가 주력으로 다루는 백엔드 시스템 아키텍처 역량을 고도화하여, 대규모 트래픽 분산 처리 및 고가용성 설계 전문가가 되는 것입니다. 특히, MSA(Microservices Architecture) 환경에서 서비스 간 트랜잭션 일관성 유지 및 분산 추적 시스템(Distributed Tracing) 구축 경험을 심화하고 싶습니다. 이를 위해 특정 클라우드 플랫폼(예: AWS Solution Architect Professional) 인증 취득 및 관련 오픈소스 프로젝트에 기여하는 것을 목표로 하고 있습니다. 둘째, 데이터 기반 의사결정 역량을 강화하여, 개발하는 서비스의 사용자 행동 패턴 분석 및 비즈니스 성과 지표 도출에 직접적으로 기여하고자 합니다. 이를 위해 데이터 파이프라인 구축 및 시각화 도구 활용 능력을 향상시키고, AB 테스트 설계 및 분석 리더 역할을 수행하고 싶습니다. 셋째, 기술 리더십 역량을 키워 주니어 개발자 멘토링 및 팀 내 기술 스택 도입을 주도하는 역할을 하고 싶습니다. 단순히 코드를 잘 짜는 것을 넘어, 팀원들의 성장을 돕고 기술 부채를 줄이며, 장기적인 아키텍처 방향성을 제시하는 시니어 엔지니어로 자리매김하고 싶습니다. 이 목표들을 달성하기 위해 지속적으로 학습하고, 회사에 실질적인 가치를 제공할 수 있도록 노력할 것입니다.', 'role': 'user'}, {'content': '{\\n  \"score\": 4.8,\\n  \"feedback\": \"3년 성장 계획을 \\'세 가지 목표\\'로 명확히 구조화하여 제시한 점이 매우 인상 깊습니다. 각 목표가 백엔드 아키텍처 고도화, 데이터 기반 의사결정 역량 강화, 기술 리더십 역량 강화 등 시니어 엔지니어로서 필요한 핵심 역량을 고르게 아우르고 있습니다. 특히, MSA 환경에서의 트랜잭션 일관성, 분산 추적 시스템 구축, AWS Solution Architect Professional 인증 취득, AB 테스트 설계 및 분석 리더 역할 수행 등 구체적인 기술 스택과 달성 방법론까지 제시하여 논리성과 구체성이 뛰어납니다. 단순히 기술 역량 향상에 그치지 않고, 비즈니스 성과 기여 및 팀원 성장 멘토링, 기술 부채 관리 등 \\'시니어\\'의 역할을 정확히 이해하고 있다는 점이 직무 적합성에 매우 긍정적인 평가를 주었습니다. 지원자의 성장 의지와 계획의 깊이가 돋보이는 답변입니다.\",\\n  \"improve\": \"제시된 성장 계획의 내용과 깊이는 매우 훌륭합니다. 한 가지 더 보완할 점이 있다면, 이러한 성장 목표들이 지원한 회사의 특정 비전이나 현재 당면한 기술적, 비즈니스적 도전과제와 어떻게 시너지를 낼 수 있을지에 대한 연결고리를 더욱 명확히 제시한다면, 단순히 개인의 성장을 넘어 \\'이 회사에서\\' 어떤 가치를 창출할지에 대한 의지를 더욱 부각시킬 수 있을 것입니다. (예: \\'저의 고가용성 아키텍처 전문성은 회사의 특정 서비스 확장 계획에 기여할 수 있습니다.\\')\"\\n}', 'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋\n",
    "training_data = create_format_dataset(interview_json_data)\n",
    "# Hugging Face Dataset 객체로 변환\n",
    "dataset = Dataset.from_pandas(pd.DataFrame(training_data))\n",
    "\n",
    "# 변환된 데이터 확인\n",
    "print(dataset[0]['messages'])\n",
    "\n",
    "train_test_split = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "# DatasetDict 생성\n",
    "split_dataset = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'test': train_test_split['test']\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-08-19T10:33:59.095695Z",
     "iopub.status.busy": "2025-08-19T10:33:59.095485Z",
     "iopub.status.idle": "2025-08-19T10:33:59.225882Z",
     "shell.execute_reply": "2025-08-19T10:33:59.225320Z",
     "shell.execute_reply.started": "2025-08-19T10:33:59.095679Z"
    },
    "executionInfo": {
     "elapsed": 160045,
     "status": "ok",
     "timestamp": 1755503919950,
     "user": {
      "displayName": "권순민",
      "userId": "01464331670350596051"
     },
     "user_tz": -540
    },
    "id": "3Ahh-_0K3ffL",
    "outputId": "98faabf4-3386-4eeb-a388-451c1449543a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# HuggingFace 로그인\n",
    "from huggingface_hub import login\n",
    "login(token=os.getenv(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T10:33:59.226832Z",
     "iopub.status.busy": "2025-08-19T10:33:59.226594Z",
     "iopub.status.idle": "2025-08-19T10:33:59.230442Z",
     "shell.execute_reply": "2025-08-19T10:33:59.229770Z",
     "shell.execute_reply.started": "2025-08-19T10:33:59.226806Z"
    },
    "executionInfo": {
     "elapsed": 83,
     "status": "ok",
     "timestamp": 1755503920045,
     "user": {
      "displayName": "권순민",
      "userId": "01464331670350596051"
     },
     "user_tz": -540
    },
    "id": "9B45mTSnsJpd",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 1. 모델 및 토크나이저 설정\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T10:33:59.231476Z",
     "iopub.status.busy": "2025-08-19T10:33:59.231196Z",
     "iopub.status.idle": "2025-08-19T10:33:59.250760Z",
     "shell.execute_reply": "2025-08-19T10:33:59.250191Z",
     "shell.execute_reply.started": "2025-08-19T10:33:59.231454Z"
    },
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1755503920050,
     "user": {
      "displayName": "권순민",
      "userId": "01464331670350596051"
     },
     "user_tz": -540
    },
    "id": "Lay3T0QJ4TYF",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 2. QLoRA 설정 (VRAM 최소화)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "de0489048ef7491f8e8f963b20541100",
      "796a599c6e024295ae4337426941a715",
      "09110c5f481f4fcbb706c757d11a4c71",
      "e93da9ce67bb4bb7aeaf2ce13ade20e8",
      "2b9c59601f384259adbcfc422a41a871",
      "5acffa625dac4e91a1d986f4512a6f37",
      "2a651eb713f94d93bdd6ee0c9fc5089b",
      "e4eba35624cb4ce8a284b5147a91482b",
      "9045053fd1494aa5bce829ef5f473777",
      "9d8b379dbe0045d48fe694d8a39f2269",
      "fff015232898458aac512c6689475982"
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-08-19T10:33:59.251698Z",
     "iopub.status.busy": "2025-08-19T10:33:59.251497Z",
     "iopub.status.idle": "2025-08-19T10:37:04.285272Z",
     "shell.execute_reply": "2025-08-19T10:37:04.284601Z",
     "shell.execute_reply.started": "2025-08-19T10:33:59.251683Z"
    },
    "executionInfo": {
     "elapsed": 90570,
     "status": "ok",
     "timestamp": 1755504010604,
     "user": {
      "displayName": "권순민",
      "userId": "01464331670350596051"
     },
     "user_tz": -540
    },
    "id": "jwMyjyob4zoh",
    "outputId": "88f62ac4-e6f2-4734-85ad-d5f69e458bbc",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1df52f58aad24f65885dd767637b2b03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd86d7abbda64afdbbc8e5352d24a459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80b7cca5ea3a40ea95e333c11d6bd91c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d044342906346a2b9fb71749865abd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09e83738a690400b817b6075381c1b4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2274f0c7d30746a5b44040b17f257874",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52cf26a7d87746769e1daa4e456f068c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "131dbcd9eb7648918122fd285223e8ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3554bfba27664197888f5f14a9ae9c34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c93ef6be05e042f2a13e3a50e903ff34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab7516f9ace44be0932db9b3ee19f23f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "094461420678424a84bf0ab43910cc9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3. 모델 및 토크나이저 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T10:37:04.286402Z",
     "iopub.status.busy": "2025-08-19T10:37:04.286148Z",
     "iopub.status.idle": "2025-08-19T10:37:04.311358Z",
     "shell.execute_reply": "2025-08-19T10:37:04.310580Z",
     "shell.execute_reply.started": "2025-08-19T10:37:04.286383Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 템플릿 적용 결과 ---\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "당신은 전문 면접 평가자입니다. 당신의 임무는 제공된 면접 질문과 지원자의 답변을 분석하는 것입니다.\n",
      "이 분석을 바탕으로 'score', 'feedback', 'improve'이라는 세 가지 키를 포함하는 JSON 객체를 생성해야 합니다.\n",
      "- 'score': 답을 평가하는 숫자 점수.\n",
      "- 'feedback': 답변의 강점에 대한 상세하고 건설적인 피드백.\n",
      "- 'improve': 답변에서 개선할 수 있는 사항에 대한 제안.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "### 질문:\n",
      "        지원자님께서 지금부터 삼 년 뒤까지 즉 삼 년 이내로 주요 성장 계획이나 목표가 있으신가요 이러한 목표가 있다면 무엇인지 말씀해 주시겠습니까\n",
      "        ### 답변:\n",
      "        네, 명확한 3년 성장 계획을 가지고 있습니다. 크게 세 가지 목표입니다. 첫째, 현재 제가 주력으로 다루는 백엔드 시스템 아키텍처 역량을 고도화하여, 대규모 트래픽 분산 처리 및 고가용성 설계 전문가가 되는 것입니다. 특히, MSA(Microservices Architecture) 환경에서 서비스 간 트랜잭션 일관성 유지 및 분산 추적 시스템(Distributed Tracing) 구축 경험을 심화하고 싶습니다. 이를 위해 특정 클라우드 플랫폼(예: AWS Solution Architect Professional) 인증 취득 및 관련 오픈소스 프로젝트에 기여하는 것을 목표로 하고 있습니다. 둘째, 데이터 기반 의사결정 역량을 강화하여, 개발하는 서비스의 사용자 행동 패턴 분석 및 비즈니스 성과 지표 도출에 직접적으로 기여하고자 합니다. 이를 위해 데이터 파이프라인 구축 및 시각화 도구 활용 능력을 향상시키고, AB 테스트 설계 및 분석 리더 역할을 수행하고 싶습니다. 셋째, 기술 리더십 역량을 키워 주니어 개발자 멘토링 및 팀 내 기술 스택 도입을 주도하는 역할을 하고 싶습니다. 단순히 코드를 잘 짜는 것을 넘어, 팀원들의 성장을 돕고 기술 부채를 줄이며, 장기적인 아키텍처 방향성을 제시하는 시니어 엔지니어로 자리매김하고 싶습니다. 이 목표들을 달성하기 위해 지속적으로 학습하고, 회사에 실질적인 가치를 제공할 수 있도록 노력할 것입니다.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "{\n",
      "  \"score\": 4.8,\n",
      "  \"feedback\": \"3년 성장 계획을 '세 가지 목표'로 명확히 구조화하여 제시한 점이 매우 인상 깊습니다. 각 목표가 백엔드 아키텍처 고도화, 데이터 기반 의사결정 역량 강화, 기술 리더십 역량 강화 등 시니어 엔지니어로서 필요한 핵심 역량을 고르게 아우르고 있습니다. 특히, MSA 환경에서의 트랜잭션 일관성, 분산 추적 시스템 구축, AWS Solution Architect Professional 인증 취득, AB 테스트 설계 및 분석 리더 역할 수행 등 구체적인 기술 스택과 달성 방법론까지 제시하여 논리성과 구체성이 뛰어납니다. 단순히 기술 역량 향상에 그치지 않고, 비즈니스 성과 기여 및 팀원 성장 멘토링, 기술 부채 관리 등 '시니어'의 역할을 정확히 이해하고 있다는 점이 직무 적합성에 매우 긍정적인 평가를 주었습니다. 지원자의 성장 의지와 계획의 깊이가 돋보이는 답변입니다.\",\n",
      "  \"improve\": \"제시된 성장 계획의 내용과 깊이는 매우 훌륭합니다. 한 가지 더 보완할 점이 있다면, 이러한 성장 목표들이 지원한 회사의 특정 비전이나 현재 당면한 기술적, 비즈니스적 도전과제와 어떻게 시너지를 낼 수 있을지에 대한 연결고리를 더욱 명확히 제시한다면, 단순히 개인의 성장을 넘어 '이 회사에서' 어떤 가치를 창출할지에 대한 의지를 더욱 부각시킬 수 있을 것입니다. (예: '저의 고가용성 아키텍처 전문성은 회사의 특정 서비스 확장 계획에 기여할 수 있습니다.')\"\n",
      "}<|eot_id|>\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "# 학습 시작 전, 검증용 코드\n",
    "# dataset은 Dataset 객체\n",
    "sample_messages = dataset[0]['messages']\n",
    "\n",
    "formatted_text = tokenizer.apply_chat_template(\n",
    "    sample_messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=False\n",
    ")\n",
    "\n",
    "print(\"--- 템플릿 적용 결과 ---\")\n",
    "print(formatted_text)\n",
    "print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T15:39:46.354559Z",
     "iopub.status.busy": "2025-08-19T15:39:46.353820Z",
     "iopub.status.idle": "2025-08-19T15:39:46.391415Z",
     "shell.execute_reply": "2025-08-19T15:39:46.390812Z",
     "shell.execute_reply.started": "2025-08-19T15:39:46.354531Z"
    },
    "executionInfo": {
     "elapsed": 49,
     "status": "ok",
     "timestamp": 1755504065742,
     "user": {
      "displayName": "권순민",
      "userId": "01464331670350596051"
     },
     "user_tz": -540
    },
    "id": "FztGF6aBsOoM",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 5. Trainer 설정\n",
    "output_dir = \"/kaggle/working/llama_interview_model\"\n",
    "training_args = SFTConfig(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=16,\n",
    "    # learning_rate=5e-6,# 학습률\n",
    "    learning_rate=2e-5,           # <-- 더 낮은 학습률\n",
    "    max_grad_norm=1.0,            # <-- 경사 클리핑 추가\n",
    "    # fp16=True,\n",
    "    logging_steps=5,\n",
    "    optim=\"paged_adamw_8bit\",  # QLora 양자화 설정 (VRAM 부족한 경우)\n",
    "    # optim=\"adamw_torch\",\n",
    "    dataset_text_field=\"messages\", # 데이터셋에서 'messages' 필드를 사용하도록 지정\n",
    "    packing=False,\n",
    "    max_length=1024,           # 시퀀스 최대 길이\n",
    "\n",
    "    # --- 스케줄러 워밍업 ---\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "\n",
    "    # --- 체크포인트 저장 설정 (스텝 기반) ---\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=20,          # 20 스텝마다 저장\n",
    "    save_total_limit=5,     # 최대 5개의 체크포인트만 유지\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "execution": {
     "iopub.execute_input": "2025-08-19T10:37:04.361961Z",
     "iopub.status.busy": "2025-08-19T10:37:04.361221Z",
     "iopub.status.idle": "2025-08-19T10:37:04.392272Z",
     "shell.execute_reply": "2025-08-19T10:37:04.391740Z",
     "shell.execute_reply.started": "2025-08-19T10:37:04.361934Z"
    },
    "executionInfo": {
     "elapsed": 200,
     "status": "error",
     "timestamp": 1755504074291,
     "user": {
      "displayName": "권순민",
      "userId": "01464331670350596051"
     },
     "user_tz": -540
    },
    "id": "0v0qYGbcyZou",
    "outputId": "9e19bb82-4214-4dd9-b818-ee7f41d64906",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 4. LoRA 설정\n",
    "lora_config = LoraConfig(\n",
    "    r=16, lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "# peft_model = get_peft_model(model, lora_config)\n",
    "# peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T10:37:04.393090Z",
     "iopub.status.busy": "2025-08-19T10:37:04.392921Z",
     "iopub.status.idle": "2025-08-19T10:37:08.898471Z",
     "shell.execute_reply": "2025-08-19T10:37:08.897651Z",
     "shell.execute_reply.started": "2025-08-19T10:37:04.393076Z"
    },
    "executionInfo": {
     "elapsed": 256236,
     "status": "aborted",
     "timestamp": 1755504010683,
     "user": {
      "displayName": "권순민",
      "userId": "01464331670350596051"
     },
     "user_tz": -540
    },
    "id": "TM3p_HOrsP5y",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6323e9bf3454c97b00c427b1b024ebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/1157 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8faf7a5d1bba4b4c8b2b0b5e05ce2f9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/1157 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22d7d6a3eb0a487bba587602202886ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/129 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce8bed1bd9204c7f8821146799e8c8d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/129 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=split_dataset['train'], # <-- 훈련 데이터셋\n",
    "    eval_dataset=split_dataset['test'],   # <-- 평가 데이터셋\n",
    "    processing_class=tokenizer,\n",
    "    peft_config=lora_config,    # LoRA 설정을 여기에 전달\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T10:37:08.899556Z",
     "iopub.status.busy": "2025-08-19T10:37:08.899320Z",
     "iopub.status.idle": "2025-08-19T10:37:08.902998Z",
     "shell.execute_reply": "2025-08-19T10:37:08.902279Z",
     "shell.execute_reply.started": "2025-08-19T10:37:08.899539Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T10:37:08.904236Z",
     "iopub.status.busy": "2025-08-19T10:37:08.903937Z",
     "iopub.status.idle": "2025-08-19T10:37:09.113820Z",
     "shell.execute_reply": "2025-08-19T10:37:09.113215Z",
     "shell.execute_reply.started": "2025-08-19T10:37:08.904213Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "secret_value_0 = user_secrets.get_secret(\"WANDB_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T10:37:09.114694Z",
     "iopub.status.busy": "2025-08-19T10:37:09.114462Z",
     "iopub.status.idle": "2025-08-19T10:37:15.348139Z",
     "shell.execute_reply": "2025-08-19T10:37:15.347586Z",
     "shell.execute_reply.started": "2025-08-19T10:37:09.114677Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msogno6037\u001b[0m (\u001b[33msogno6037-university-of-ulsan\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=secret_value_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T15:39:08.773604Z",
     "iopub.status.busy": "2025-08-19T15:39:08.773286Z",
     "iopub.status.idle": "2025-08-19T15:39:08.824502Z",
     "shell.execute_reply": "2025-08-19T15:39:08.823377Z",
     "shell.execute_reply.started": "2025-08-19T15:39:08.773582Z"
    },
    "executionInfo": {
     "elapsed": 256236,
     "status": "aborted",
     "timestamp": 1755504010686,
     "user": {
      "displayName": "권순민",
      "userId": "01464331670350596051"
     },
     "user_tz": -540
    },
    "id": "6E7BLY8_yfjo",
    "trusted": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No valid checkpoint found in output directory (/kaggle/working/llama_interview_model)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36/1726826060.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 학습 시작 ( resume_from_checkpoint=True )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 학습된 모델 저장\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/working/final-llama3-model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2204\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_last_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2205\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresume_from_checkpoint\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2206\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"No valid checkpoint found in output directory ({args.output_dir})\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2208\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresume_from_checkpoint\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No valid checkpoint found in output directory (/kaggle/working/llama_interview_model)"
     ]
    }
   ],
   "source": [
    "# 학습 시작 ( resume_from_checkpoint=True )\n",
    "trainer.train()\n",
    "\n",
    "# 학습된 모델 저장\n",
    "trainer.save_model(\"/kaggle/working/final-llama3-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T15:37:08.568072Z",
     "iopub.status.busy": "2025-08-19T15:37:08.567475Z",
     "iopub.status.idle": "2025-08-19T15:37:08.839042Z",
     "shell.execute_reply": "2025-08-19T15:37:08.838321Z",
     "shell.execute_reply.started": "2025-08-19T15:37:08.568051Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: kaggle/working/ (stored 0%)\n",
      "  adding: kaggle/working/llama_interview_model/ (stored 0%)\n",
      "  adding: kaggle/working/llama_interview_model/runs/ (stored 0%)\n",
      "  adding: kaggle/working/llama_interview_model/runs/Aug19_10-37-04_143a3fbda84c/ (stored 0%)\n",
      "  adding: kaggle/working/llama_interview_model/runs/Aug19_10-37-04_143a3fbda84c/events.out.tfevents.1755599835.143a3fbda84c.36.0 (deflated 61%)\n",
      "  adding: kaggle/working/wandb/ (stored 0%)\n",
      "  adding: kaggle/working/wandb/run-20250819_103715-rpn09rgv/ (stored 0%)\n",
      "  adding: kaggle/working/wandb/run-20250819_103715-rpn09rgv/run-rpn09rgv.wandb (deflated 89%)\n",
      "  adding: kaggle/working/wandb/run-20250819_103715-rpn09rgv/tmp/ (stored 0%)\n",
      "  adding: kaggle/working/wandb/run-20250819_103715-rpn09rgv/tmp/code/ (stored 0%)\n",
      "  adding: kaggle/working/wandb/run-20250819_103715-rpn09rgv/logs/ (stored 0%)\n",
      "  adding: kaggle/working/wandb/run-20250819_103715-rpn09rgv/logs/debug.log (deflated 66%)\n",
      "  adding: kaggle/working/wandb/run-20250819_103715-rpn09rgv/logs/debug-core.log (deflated 57%)\n",
      "  adding: kaggle/working/wandb/run-20250819_103715-rpn09rgv/logs/debug-internal.log (deflated 66%)\n",
      "  adding: kaggle/working/wandb/run-20250819_103715-rpn09rgv/files/ (stored 0%)\n",
      "  adding: kaggle/working/wandb/run-20250819_103715-rpn09rgv/files/requirements.txt (deflated 56%)\n",
      "  adding: kaggle/working/wandb/run-20250819_103715-rpn09rgv/files/wandb-metadata.json (deflated 53%)\n",
      "  adding: kaggle/working/wandb/run-20250819_103715-rpn09rgv/files/output.log (deflated 39%)\n",
      "  adding: kaggle/working/wandb/debug.log (deflated 66%)\n",
      "  adding: kaggle/working/wandb/debug-internal.log (deflated 66%)\n",
      "  adding: kaggle/working/wandb/latest-run/ (stored 0%)\n",
      "  adding: kaggle/working/wandb/latest-run/run-rpn09rgv.wandb (deflated 89%)\n",
      "  adding: kaggle/working/wandb/latest-run/tmp/ (stored 0%)\n",
      "  adding: kaggle/working/wandb/latest-run/tmp/code/ (stored 0%)\n",
      "  adding: kaggle/working/wandb/latest-run/logs/ (stored 0%)\n",
      "  adding: kaggle/working/wandb/latest-run/logs/debug.log (deflated 66%)\n",
      "  adding: kaggle/working/wandb/latest-run/logs/debug-core.log (deflated 57%)\n",
      "  adding: kaggle/working/wandb/latest-run/logs/debug-internal.log (deflated 66%)\n",
      "  adding: kaggle/working/wandb/latest-run/files/ (stored 0%)\n",
      "  adding: kaggle/working/wandb/latest-run/files/requirements.txt (deflated 56%)\n",
      "  adding: kaggle/working/wandb/latest-run/files/wandb-metadata.json (deflated 53%)\n",
      "  adding: kaggle/working/wandb/latest-run/files/output.log (deflated 69%)\n",
      "  adding: kaggle/working/.virtual_documents/ (stored 0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# !zip -r output.zip /kaggle/working"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "authorship_tag": "ABX9TyP9mSsbX3vAr8iqrLTT/9aq",
   "gpuType": "V28",
   "mount_file_id": "1HPC0Cc4B1J_TE1P_AYPnxDh9IshwNHVz",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8089680,
     "sourceId": 12795313,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8099288,
     "sourceId": 12808895,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "09110c5f481f4fcbb706c757d11a4c71": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e4eba35624cb4ce8a284b5147a91482b",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9045053fd1494aa5bce829ef5f473777",
      "value": 4
     }
    },
    "2a651eb713f94d93bdd6ee0c9fc5089b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2b9c59601f384259adbcfc422a41a871": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5acffa625dac4e91a1d986f4512a6f37": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "796a599c6e024295ae4337426941a715": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5acffa625dac4e91a1d986f4512a6f37",
      "placeholder": "​",
      "style": "IPY_MODEL_2a651eb713f94d93bdd6ee0c9fc5089b",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "9045053fd1494aa5bce829ef5f473777": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9d8b379dbe0045d48fe694d8a39f2269": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "de0489048ef7491f8e8f963b20541100": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_796a599c6e024295ae4337426941a715",
       "IPY_MODEL_09110c5f481f4fcbb706c757d11a4c71",
       "IPY_MODEL_e93da9ce67bb4bb7aeaf2ce13ade20e8"
      ],
      "layout": "IPY_MODEL_2b9c59601f384259adbcfc422a41a871"
     }
    },
    "e4eba35624cb4ce8a284b5147a91482b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e93da9ce67bb4bb7aeaf2ce13ade20e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9d8b379dbe0045d48fe694d8a39f2269",
      "placeholder": "​",
      "style": "IPY_MODEL_fff015232898458aac512c6689475982",
      "value": " 4/4 [01:27&lt;00:00, 18.02s/it]"
     }
    },
    "fff015232898458aac512c6689475982": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
